{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import etree\n",
    "from io import StringIO, BytesIO\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import math\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "#time is undifined import module\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# headers = {\n",
    "#     'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.54 Safari/537.36',\n",
    "# }\n",
    "# list_link = [\"https://medhelp.org/forums/HIV-Living-With/show/97\"]\n",
    "\n",
    "# results = []\n",
    "# questions_links_results = []\n",
    "\n",
    "\n",
    "\n",
    "# for link in tqdm(list_link, desc=\"Scraping pages\"):\n",
    "#     page = 1\n",
    "#     number_of_questions = 0\n",
    "\n",
    "#     while page <10:\n",
    "#     #i want to print \" page x currenty scraping\" where it just updates in the same line instead of printing a new line\n",
    "#         print(\"page \" + str(page) + \" currently scraping\", end=\"\\r\")\n",
    "        \n",
    "#         link_with_page = link + \"?page=\" + str(page)\n",
    "#         r = requests.get(link_with_page, headers=headers)\n",
    "#         soup = BeautifulSoup(r.text, 'html.parser')\n",
    "#         divs = soup.find_all('div', {'class': 'subj_entry'})\n",
    "\n",
    "#         number_of_questions += len(divs)\n",
    "#         page += 1\n",
    "        \n",
    "#         # #extract the html links from the page,there will be multiple links, which is formated as follows:\n",
    "#         # <div class=\"subj_info\">\n",
    "#         #   <h2 class=\"subj_title \">\n",
    "#         #       <a href=\"https://medhelp.org/posts/HIV-Living-With/CD4-count/show/3051908\">CD4 count</a>\n",
    "#         link_questions = etree.HTML(r.text).xpath('//div[@class=\"subj_info\"]/h2[@class=\"subj_title \"]/a/@href')\n",
    "#         #print(link_question)\n",
    "#         for link_question in link_questions:\n",
    "#             link_questions_html = \"https://medhelp.org\" + link_question\n",
    "#             #print(link_questions_html, end=\"\\n\")\n",
    "#             tmp = {'url': link_questions_html}\n",
    "#             questions_links_results += [tmp]\n",
    "\n",
    "#             # with open(progress_file, 'w') as file:\n",
    "#             #     file.write(link_questions_html)\n",
    "            \n",
    "#             # if start_from and link_questions_html != start_from:\n",
    "#             #     continue\n",
    "#         if len(divs) < 4:\n",
    "#             break\n",
    "    \n",
    "#     for questions_links_result in questions_links_results:\n",
    "#         #i want to print \"question x/total currenty scraping \" where it just updates in the same line instead of printing a new line\n",
    "#         print(\"question \" + str(questions_links_results.index(questions_links_result)+1) + \"/\" + str(len(questions_links_results)) + \" currently scraping\", end=\"\\r\")\n",
    "#         r = requests.get(questions_links_result['url'], headers=headers)\n",
    "\n",
    "#         question_user_name = etree.HTML(r.text).xpath('//div[@class=\"subj_header\"]/div[@class=\"subj_info\"]/div[@class=\"username\"]/a/text()')\n",
    "#         question_user_id = etree.HTML(r.text).xpath('//div[@class=\"subj_header\"]/div[@class=\"subj_info\"]/div[@class=\"username\"]/a/@href')\n",
    "#         question_msg = etree.HTML(r.text).xpath('//div[@id=\"subject_msg\"]/text()')\n",
    "#         question_msg = [question_msg] # Convert to list so that it matches the format of the answer data\n",
    "#         question_timestamp = etree.HTML(r.text).xpath('//div[@class=\"subj_header\"]/div[@class=\"subj_info\"]/div[@class=\"username\"]/time/@data-timestamp')\n",
    "\n",
    "#         elements = etree.HTML(r.text).xpath(\"//*[starts-with(@class, 'mh_vit_resp_ctn')]\")\n",
    "\n",
    "#         def clean_text(text):\n",
    "#             # Remove <br>, \\n, and \\r tags, and strip leading/trailing whitespace\n",
    "#             cleaned_text = re.sub(r'<br>|[\\n\\r]', '', text).strip()\n",
    "#             return cleaned_text\n",
    "        \n",
    "#         if len(elements) == 0:\n",
    "#             item = {}\n",
    "#             item['question_url'] = questions_links_result['url']\n",
    "#             item['question_user_name'] = question_user_name[0] if question_user_name else None\n",
    "#             item['question_user_id'] = question_user_id[0] if question_user_id else None\n",
    "#             item['question_msg'] = question_msg[0] if question_msg else None\n",
    "#             item['question_timestamp'] = question_timestamp[0] if question_timestamp else None\n",
    "#             item['ans_users_name'] = [\"No Answers\"]\n",
    "#             item['ans_user_id'] = [\"No Answers\"]\n",
    "#             item['ans_msg'] = [\"No Answers\"]\n",
    "#             item['ans_timestamp'] = [\"No Answers\"]\n",
    "#             item['comment_users_name'] = [\"No Comments\"]\n",
    "#             item['comment_user_id'] = [\"No Comments\"]\n",
    "#             item['comment_msg'] = [\"No Comments\"]\n",
    "#             item['comment_timestamp'] = [\"No Comments\"]\n",
    "#             results.append(item)\n",
    "         \n",
    "\n",
    "#         for element in elements:\n",
    "#             item = {}\n",
    "            \n",
    "#             # Page answer data\n",
    "#             ans_users_name = element.xpath(\".//div[@class='resp_header']//div[@class='username']/a/text()\")\n",
    "#             ans_user_id = element.xpath(\".//div[@class='resp_header']//div[@class='username']/a/@href\")\n",
    "#             ans_user_id = ans_user_id[0] if ans_user_id else None\n",
    "#             ans_msg = element.xpath(\".//div[@class='resp_body ']/text()\")  # Assuming this is the answer text\n",
    "#             ans_timestamp = element.xpath(\".//div[@class='resp_header']//div[@class='username']/time/@data-timestamp\")\n",
    "\n",
    "#             item['question_url'] = questions_links_result['url']\n",
    "#             item['question_user_name'] = question_user_name[0] if question_user_name else None\n",
    "#             item['question_user_id'] = question_user_id[0] if question_user_id else None\n",
    "#             item['question_msg'] = question_msg[0] if question_msg else None\n",
    "#             item['question_timestamp'] = question_timestamp[0] if question_timestamp else None\n",
    "\n",
    "#             item['ans_users_name'] = ans_users_name\n",
    "#             item['ans_user_id'] = ans_user_id\n",
    "#             item['ans_msg'] = ans_msg\n",
    "#             item['ans_timestamp'] = ans_timestamp\n",
    "\n",
    "#             # Page comment data\n",
    "#             comment_elements = element.xpath(\".//div[@class='comment_ctn']\")\n",
    "            \n",
    "#             if len(comment_elements) == 0:\n",
    "#                 item['comment_users_name'] = [\"No Comments\"]\n",
    "#                 item['comment_user_id'] = [\"No Comments\"]\n",
    "#                 item['comment_msg'] = [\"No Comments\"]\n",
    "#                 item['comment_timestamp'] = [\"No Comments\"]\n",
    "#                 results.append(item)\n",
    "#             else:\n",
    "                \n",
    "#                 for i,comment_element in enumerate(comment_elements):\n",
    "#                     #print the html section that is withing the tag in comment_element\n",
    "#                     # print(etree.tostring(comment_element))\n",
    "#                     # print(f)\n",
    "#                     # f+=1\n",
    "#                     comment_users_name = comment_element.xpath(\".//div[@class='username']/a/text()\")\n",
    "#                     comment_user_id = comment_element.xpath(\".//div[@class='username']/a/@href\")\n",
    "#                     comment_timestamp = comment_element.xpath(\".//div[@class='username']/time/@data-timestamp\")\n",
    "#                     comment_user_id =  comment_user_id[0] if comment_user_id else None\n",
    "#                     comment_msg = comment_element.xpath(\".//div[@class='comment_body ']/text()\")\n",
    "\n",
    "#                     if not comment_users_name:\n",
    "#                         comment_users_name = [\"No Comments\"]\n",
    "#                         comment_user_id = [\"No Comments\"]\n",
    "#                         comment_msg = [\"No Comments\"]\n",
    "#                         comment_timestamp = [\"No Comments\"]\n",
    "#                     else:\n",
    "#                         comment_msg = [clean_text(msg) for msg in comment_msg]\n",
    "#                         #combine all the elements of the list into a single element\n",
    "#                         comment_msg = [''.join(comment_msg)]\n",
    "#                     #print(comment_msg)\n",
    "                \n",
    "#                     item['comment_users_name'] = comment_users_name\n",
    "#                     item['comment_user_id'] = comment_user_id\n",
    "#                     item['comment_msg'] = comment_msg\n",
    "#                     item['comment_timestamp'] = comment_timestamp\n",
    "#                     results.append(item.copy()) # Append a copy of the item dictionary to the result list\n",
    "#                         # print last row of the result\n",
    "#                         #print(result[-1])\n",
    "\n",
    "# df = pd.DataFrame(results)\n",
    "# df.to_csv('test_timestamp.csv', index=False)\n",
    "# df\n",
    "    \n",
    "# # Assuming 'sqlite:///test.db' is the database URL. You can replace it with your database URL.\n",
    "# engine = create_engine('sqlite:///test.db')\n",
    "\n",
    "# # Your DataFrame\n",
    "# df = pd.DataFrame(results)\n",
    "\n",
    "# # Specify the table name you want\n",
    "# table_name = 'your_table_name'\n",
    "# # Convert all columns to strings\n",
    "# df = df.astype(str)\n",
    "\n",
    "# # Store DataFrame into the database\n",
    "# df.to_sql(table_name, con=engine, index=False, if_exists='append')\n",
    "# # Now your DataFrame is stored in the specified table in the database.\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_pages(list_links, start_page=1, end_page=10, csv_filename='test_timestamp.csv', table_name='your_table_name'):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.54 Safari/537.36',\n",
    "    }\n",
    "    results = []\n",
    "\n",
    "    for link in tqdm(list_links, desc=\"Scraping pages\"):\n",
    "        page = start_page\n",
    "        questions_links_results = []\n",
    "\n",
    "        while page < end_page:\n",
    "            print(\"page \" + str(page) + \" currently scraping\", end=\"\\r\")\n",
    "            link_with_page = link + \"?page=\" + str(page)\n",
    "            #print(link_with_page)\n",
    "            r = requests.get(link_with_page, headers=headers)\n",
    "            soup = BeautifulSoup(r.text, 'html.parser')\n",
    "            divs = soup.find_all('div', {'class': 'subj_entry'})\n",
    "\n",
    "            if len(divs) < 4:\n",
    "                break\n",
    "\n",
    "            link_questions = etree.HTML(r.text).xpath('//div[@class=\"subj_info\"]/h2[@class=\"subj_title \"]/a/@href')\n",
    "            for link_question in link_questions:\n",
    "                link_questions_html = \"https://medhelp.org\" + link_question\n",
    "                tmp = {'url': link_questions_html}\n",
    "                questions_links_results.append(tmp)\n",
    "\n",
    "            page += 1\n",
    "\n",
    "        for questions_links_result in questions_links_results:\n",
    "            #i want to print \"question x/total currenty scraping \" where it just updates in the same line instead of printing a new line\n",
    "            #i want the next print to be in the next line of the previous print and keep the previous page print statement\n",
    "            #print(\"scrapping question now\", end= \"\\n\")\n",
    "            print(\"question \" + str(questions_links_results.index(questions_links_result)+1) + \"/\" + str(len(questions_links_results)) + \" currently scraping\", end=\"\\r\")\n",
    "            r = requests.get(questions_links_result['url'], headers=headers)\n",
    "\n",
    "            question_user_name = etree.HTML(r.text).xpath('//div[@class=\"subj_header\"]/div[@class=\"subj_info\"]/div[@class=\"username\"]/a/text()')\n",
    "            question_user_id = etree.HTML(r.text).xpath('//div[@class=\"subj_header\"]/div[@class=\"subj_info\"]/div[@class=\"username\"]/a/@href')\n",
    "            question_msg = etree.HTML(r.text).xpath('//div[@id=\"subject_msg\"]/text()')\n",
    "            question_msg = [question_msg] # Convert to list so that it matches the format of the answer data\n",
    "            question_timestamp = etree.HTML(r.text).xpath('//div[@class=\"subj_header\"]/div[@class=\"subj_info\"]/div[@class=\"username\"]/time/@data-timestamp')\n",
    "\n",
    "            elements = etree.HTML(r.text).xpath(\"//*[starts-with(@class, 'mh_vit_resp_ctn')]\")\n",
    "\n",
    "            def clean_text(text):\n",
    "                # Remove <br>, \\n, and \\r tags, and strip leading/trailing whitespace\n",
    "                cleaned_text = re.sub(r'<br>|[\\n\\r]', '', text).strip()\n",
    "                return cleaned_text\n",
    "            \n",
    "            if len(elements) == 0:\n",
    "                item = {}\n",
    "                item['question_url'] = questions_links_result['url']\n",
    "                item['question_user_name'] = question_user_name[0] if question_user_name else None\n",
    "                item['question_user_id'] = question_user_id[0] if question_user_id else None\n",
    "                item['question_msg'] = question_msg[0] if question_msg else None\n",
    "                item['question_timestamp'] = question_timestamp[0] if question_timestamp else None\n",
    "                item['ans_users_name'] = [\"No Answers\"]\n",
    "                item['ans_user_id'] = [\"No Answers\"]\n",
    "                item['ans_msg'] = [\"No Answers\"]\n",
    "                item['ans_timestamp'] = [\"No Answers\"]\n",
    "                item['comment_users_name'] = [\"No Comments\"]\n",
    "                item['comment_user_id'] = [\"No Comments\"]\n",
    "                item['comment_msg'] = [\"No Comments\"]\n",
    "                item['comment_timestamp'] = [\"No Comments\"]\n",
    "                results.append(item)\n",
    "            \n",
    "\n",
    "            for element in elements:\n",
    "                item = {}\n",
    "                \n",
    "                # Page answer data\n",
    "                ans_users_name = element.xpath(\".//div[@class='resp_header']//div[@class='username']/a/text()\")\n",
    "                ans_user_id = element.xpath(\".//div[@class='resp_header']//div[@class='username']/a/@href\")\n",
    "                ans_user_id = ans_user_id[0] if ans_user_id else None\n",
    "                ans_msg = element.xpath(\".//div[@class='resp_body ']/text()\")  # Assuming this is the answer text\n",
    "                ans_timestamp = element.xpath(\".//div[@class='resp_header']//div[@class='username']/time/@data-timestamp\")\n",
    "\n",
    "                item['question_url'] = questions_links_result['url']\n",
    "                item['question_user_name'] = question_user_name[0] if question_user_name else None\n",
    "                item['question_user_id'] = question_user_id[0] if question_user_id else None\n",
    "                item['question_msg'] = question_msg[0] if question_msg else None\n",
    "                item['question_timestamp'] = question_timestamp[0] if question_timestamp else None\n",
    "\n",
    "                item['ans_users_name'] = ans_users_name\n",
    "                item['ans_user_id'] = ans_user_id\n",
    "                item['ans_msg'] = ans_msg\n",
    "                item['ans_timestamp'] = ans_timestamp\n",
    "\n",
    "                # Page comment data\n",
    "                comment_elements = element.xpath(\".//div[@class='comment_ctn']\")\n",
    "                \n",
    "                if len(comment_elements) == 0:\n",
    "                    item['comment_users_name'] = [\"No Comments\"]\n",
    "                    item['comment_user_id'] = [\"No Comments\"]\n",
    "                    item['comment_msg'] = [\"No Comments\"]\n",
    "                    item['comment_timestamp'] = [\"No Comments\"]\n",
    "                    results.append(item)\n",
    "                else:\n",
    "                    \n",
    "                    for i,comment_element in enumerate(comment_elements):\n",
    "                        #print the html section that is withing the tag in comment_element\n",
    "                        # print(etree.tostring(comment_element))\n",
    "                        # print(f)\n",
    "                        # f+=1\n",
    "                        comment_users_name = comment_element.xpath(\".//div[@class='username']/a/text()\")\n",
    "                        comment_user_id = comment_element.xpath(\".//div[@class='username']/a/@href\")\n",
    "                        comment_timestamp = comment_element.xpath(\".//div[@class='username']/time/@data-timestamp\")\n",
    "                        comment_user_id =  comment_user_id[0] if comment_user_id else None\n",
    "                        comment_msg = comment_element.xpath(\".//div[@class='comment_body ']/text()\")\n",
    "\n",
    "                        if not comment_users_name:\n",
    "                            comment_users_name = [\"No Comments\"]\n",
    "                            comment_user_id = [\"No Comments\"]\n",
    "                            comment_msg = [\"No Comments\"]\n",
    "                            comment_timestamp = [\"No Comments\"]\n",
    "                        else:\n",
    "                            comment_msg = [clean_text(msg) for msg in comment_msg]\n",
    "                            #combine all the elements of the list into a single element\n",
    "                            comment_msg = [''.join(comment_msg)]\n",
    "                        #print(comment_msg)\n",
    "                    \n",
    "                        item['comment_users_name'] = comment_users_name\n",
    "                        item['comment_user_id'] = comment_user_id\n",
    "                        item['comment_msg'] = comment_msg\n",
    "                        item['comment_timestamp'] = comment_timestamp\n",
    "                        results.append(item.copy()) # Append a copy of the item dictionary to the result list\n",
    "                            # print last row of the result\n",
    "                            #print(result[-1])\n",
    "    df = pd.DataFrame(results)\n",
    "    if not df.empty:\n",
    "        df.to_csv(csv_filename, index=False)\n",
    "\n",
    "        engine = create_engine('sqlite:///MedhelpHerpesAuto.db')\n",
    "        table_name = table_name.replace(\"_\", '')\n",
    "        # Convert all columns to strings\n",
    "        df = df.astype(str)\n",
    "        # Store DataFrame into the database\n",
    "        df.to_sql(table_name, con=engine, index=False, if_exists='replace')\n",
    "    else:\n",
    "        print(f\"Dataframe is empty for pages {start_page} to {end_page}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question 594/594 currently scraping\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 1/1 [03:49<00:00, 229.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping pages 1 to 100 and saved to Medhelp_Anxiety_Autoscrape1100.csv and Medhelp_Anxiety_Autoscrape1100.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question 593/594 currently scraping\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 1/1 [03:53<00:00, 234.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question 594/594 currently scraping\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 1/1 [03:53<00:00, 234.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping pages 101 to 200 and saved to Medhelp_Anxiety_Autoscrape101200.csv and Medhelp_Anxiety_Autoscrape101200.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question 594/594 currently scraping\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 1/1 [04:01<00:00, 241.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping pages 201 to 300 and saved to Medhelp_Anxiety_Autoscrape201300.csv and Medhelp_Anxiety_Autoscrape201300.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question 594/594 currently scraping\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 1/1 [04:05<00:00, 245.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping pages 301 to 400 and saved to Medhelp_Anxiety_Autoscrape301400.csv and Medhelp_Anxiety_Autoscrape301400.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question 594/594 currently scraping\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 1/1 [04:00<00:00, 240.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping pages 401 to 500 and saved to Medhelp_Anxiety_Autoscrape401500.csv and Medhelp_Anxiety_Autoscrape401500.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question 594/594 currently scraping\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 1/1 [04:13<00:00, 253.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping pages 501 to 600 and saved to Medhelp_Anxiety_Autoscrape501600.csv and Medhelp_Anxiety_Autoscrape501600.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question 594/594 currently scraping\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 1/1 [04:20<00:00, 260.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping pages 601 to 700 and saved to Medhelp_Anxiety_Autoscrape601700.csv and Medhelp_Anxiety_Autoscrape601700.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question 594/594 currently scraping\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 1/1 [03:54<00:00, 234.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping pages 701 to 800 and saved to Medhelp_Anxiety_Autoscrape701800.csv and Medhelp_Anxiety_Autoscrape701800.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question 594/594 currently scraping\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 1/1 [06:53<00:00, 413.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping pages 801 to 900 and saved to Medhelp_Anxiety_Autoscrape801900.csv and Medhelp_Anxiety_Autoscrape801900.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question 594/594 currently scraping\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 1/1 [04:02<00:00, 242.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping pages 901 to 1000 and saved to Medhelp_Anxiety_Autoscrape9011000.csv and Medhelp_Anxiety_Autoscrape9011000.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question 594/594 currently scraping\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 1/1 [04:33<00:00, 273.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping pages 1001 to 1100 and saved to Medhelp_Anxiety_Autoscrape10011100.csv and Medhelp_Anxiety_Autoscrape10011100.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question 594/594 currently scraping\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 1/1 [06:23<00:00, 383.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping pages 1101 to 1200 and saved to Medhelp_Anxiety_Autoscrape11011200.csv and Medhelp_Anxiety_Autoscrape11011200.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question 594/594 currently scraping\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 1/1 [04:47<00:00, 287.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping pages 1201 to 1300 and saved to Medhelp_Anxiety_Autoscrape12011300.csv and Medhelp_Anxiety_Autoscrape12011300.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question 594/594 currently scraping\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 1/1 [04:51<00:00, 291.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping pages 1301 to 1400 and saved to Medhelp_Anxiety_Autoscrape13011400.csv and Medhelp_Anxiety_Autoscrape13011400.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question 594/594 currently scraping\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 1/1 [04:38<00:00, 278.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping pages 1401 to 1500 and saved to Medhelp_Anxiety_Autoscrape14011500.csv and Medhelp_Anxiety_Autoscrape14011500.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question 594/594 currently scraping\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 1/1 [05:01<00:00, 301.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping pages 1501 to 1600 and saved to Medhelp_Anxiety_Autoscrape15011600.csv and Medhelp_Anxiety_Autoscrape15011600.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question 594/594 currently scraping\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 1/1 [04:38<00:00, 278.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping pages 1601 to 1700 and saved to Medhelp_Anxiety_Autoscrape16011700.csv and Medhelp_Anxiety_Autoscrape16011700.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question 594/594 currently scraping\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 1/1 [04:10<00:00, 250.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping pages 1701 to 1800 and saved to Medhelp_Anxiety_Autoscrape17011800.csv and Medhelp_Anxiety_Autoscrape17011800.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question 594/594 currently scraping\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 1/1 [04:32<00:00, 272.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping pages 1801 to 1900 and saved to Medhelp_Anxiety_Autoscrape18011900.csv and Medhelp_Anxiety_Autoscrape18011900.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question 594/594 currently scraping\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 1/1 [04:44<00:00, 284.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping pages 1901 to 2000 and saved to Medhelp_Anxiety_Autoscrape19012000.csv and Medhelp_Anxiety_Autoscrape19012000.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question 594/594 currently scraping\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 1/1 [04:30<00:00, 270.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping pages 2001 to 2100 and saved to Medhelp_Anxiety_Autoscrape20012100.csv and Medhelp_Anxiety_Autoscrape20012100.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question 594/594 currently scraping\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 1/1 [04:36<00:00, 276.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping pages 2101 to 2200 and saved to Medhelp_Anxiety_Autoscrape21012200.csv and Medhelp_Anxiety_Autoscrape21012200.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question 594/594 currently scraping\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 1/1 [04:36<00:00, 276.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping pages 2201 to 2300 and saved to Medhelp_Anxiety_Autoscrape22012300.csv and Medhelp_Anxiety_Autoscrape22012300.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question 594/594 currently scraping\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 1/1 [05:12<00:00, 312.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping pages 2301 to 2400 and saved to Medhelp_Anxiety_Autoscrape23012400.csv and Medhelp_Anxiety_Autoscrape23012400.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question 594/594 currently scraping\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 1/1 [06:12<00:00, 372.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping pages 2401 to 2500 and saved to Medhelp_Anxiety_Autoscrape24012500.csv and Medhelp_Anxiety_Autoscrape24012500.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question 594/594 currently scraping\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 1/1 [05:54<00:00, 354.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping pages 2501 to 2600 and saved to Medhelp_Anxiety_Autoscrape25012600.csv and Medhelp_Anxiety_Autoscrape25012600.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question 594/594 currently scraping\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 1/1 [06:09<00:00, 369.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping pages 2601 to 2700 and saved to Medhelp_Anxiety_Autoscrape26012700.csv and Medhelp_Anxiety_Autoscrape26012700.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question 594/594 currently scraping\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 1/1 [05:54<00:00, 354.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping pages 2701 to 2800 and saved to Medhelp_Anxiety_Autoscrape27012800.csv and Medhelp_Anxiety_Autoscrape27012800.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question 594/594 currently scraping\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 1/1 [05:37<00:00, 337.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping pages 2801 to 2900 and saved to Medhelp_Anxiety_Autoscrape28012900.csv and Medhelp_Anxiety_Autoscrape28012900.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question 594/594 currently scraping\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 1/1 [05:52<00:00, 352.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping pages 2901 to 3000 and saved to Medhelp_Anxiety_Autoscrape29013000.csv and Medhelp_Anxiety_Autoscrape29013000.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question 594/594 currently scraping\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 1/1 [05:41<00:00, 341.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping pages 3001 to 3100 and saved to Medhelp_Anxiety_Autoscrape30013100.csv and Medhelp_Anxiety_Autoscrape30013100.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question 594/594 currently scraping\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 1/1 [05:30<00:00, 330.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping pages 3101 to 3200 and saved to Medhelp_Anxiety_Autoscrape31013200.csv and Medhelp_Anxiety_Autoscrape31013200.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question 594/594 currently scraping\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 1/1 [05:32<00:00, 332.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping pages 3201 to 3300 and saved to Medhelp_Anxiety_Autoscrape32013300.csv and Medhelp_Anxiety_Autoscrape32013300.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question 594/594 currently scraping\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 1/1 [05:35<00:00, 335.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping pages 3301 to 3400 and saved to Medhelp_Anxiety_Autoscrape33013400.csv and Medhelp_Anxiety_Autoscrape33013400.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question 594/594 currently scraping\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 1/1 [07:48<00:00, 468.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping pages 3401 to 3500 and saved to Medhelp_Anxiety_Autoscrape34013500.csv and Medhelp_Anxiety_Autoscrape34013500.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question 594/594 currently scraping\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 1/1 [07:53<00:00, 473.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping pages 3501 to 3600 and saved to Medhelp_Anxiety_Autoscrape35013600.csv and Medhelp_Anxiety_Autoscrape35013600.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question 594/594 currently scraping\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 1/1 [06:46<00:00, 406.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping pages 3601 to 3700 and saved to Medhelp_Anxiety_Autoscrape36013700.csv and Medhelp_Anxiety_Autoscrape36013700.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question 594/594 currently scraping\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 1/1 [06:48<00:00, 408.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping pages 3701 to 3800 and saved to Medhelp_Anxiety_Autoscrape37013800.csv and Medhelp_Anxiety_Autoscrape37013800.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question 594/594 currently scraping\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 1/1 [06:36<00:00, 396.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping pages 3801 to 3900 and saved to Medhelp_Anxiety_Autoscrape38013900.csv and Medhelp_Anxiety_Autoscrape38013900.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question 594/594 currently scraping\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 1/1 [06:35<00:00, 395.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping pages 3901 to 4000 and saved to Medhelp_Anxiety_Autoscrape39014000.csv and Medhelp_Anxiety_Autoscrape39014000.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question 594/594 currently scraping\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 1/1 [06:27<00:00, 387.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping pages 4001 to 4100 and saved to Medhelp_Anxiety_Autoscrape40014100.csv and Medhelp_Anxiety_Autoscrape40014100.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question 594/594 currently scraping\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 1/1 [06:27<00:00, 387.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping pages 4101 to 4200 and saved to Medhelp_Anxiety_Autoscrape41014200.csv and Medhelp_Anxiety_Autoscrape41014200.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question 594/594 currently scraping\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 1/1 [05:33<00:00, 333.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping pages 4201 to 4300 and saved to Medhelp_Anxiety_Autoscrape42014300.csv and Medhelp_Anxiety_Autoscrape42014300.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question 594/594 currently scraping\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 1/1 [05:52<00:00, 352.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping pages 4301 to 4400 and saved to Medhelp_Anxiety_Autoscrape43014400.csv and Medhelp_Anxiety_Autoscrape43014400.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question 594/594 currently scraping\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 1/1 [07:07<00:00, 427.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping pages 4401 to 4500 and saved to Medhelp_Anxiety_Autoscrape44014500.csv and Medhelp_Anxiety_Autoscrape44014500.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question 594/594 currently scraping\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 1/1 [06:55<00:00, 415.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping pages 4501 to 4600 and saved to Medhelp_Anxiety_Autoscrape45014600.csv and Medhelp_Anxiety_Autoscrape45014600.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question 594/594 currently scraping\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 1/1 [06:35<00:00, 395.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping pages 4601 to 4700 and saved to Medhelp_Anxiety_Autoscrape46014700.csv and Medhelp_Anxiety_Autoscrape46014700.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question 594/594 currently scraping\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 1/1 [06:51<00:00, 411.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping pages 4701 to 4800 and saved to Medhelp_Anxiety_Autoscrape47014800.csv and Medhelp_Anxiety_Autoscrape47014800.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question 540/540 currently scraping\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 1/1 [05:02<00:00, 302.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping pages 4801 to 4900 and saved to Medhelp_Anxiety_Autoscrape48014900.csv and Medhelp_Anxiety_Autoscrape48014900.db\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "list_of_links = [\"https://www.medhelp.org/forums/Herpes/show/195\"]\n",
    "#I want to run a for loop using the scrape_pages function so it starts from page 1 end page is 1000 and keep scraping in interval of 1000 pages and saves them numbered in csv and table so page 1-1000 wilol be scrapped and saved as \"Medhelp_HIV_Autoscrape_1-1000.csv\" and \"Medhelp_HIV_Autoscrape_1-1000.db\" and then page 1001-2000 will be saved as \"Medhelp_HIV_Autoscrape_1001-2000.csv\" and \"Medhelp_HIV_Autoscrape_1001-2000.db\" and so on. until it reaches page number which i will define as final page\n",
    "final_page = 4900\n",
    "interval = 100\n",
    "for i in range(1, final_page, interval):\n",
    "    start_page = i\n",
    "    end_page = i + interval - 1\n",
    "    csv_filename = f\"Medhelp_Anxiety_Autoscrape{start_page}{end_page}.csv\"\n",
    "    table_name = f\"Medhelp_Anxiety_Autoscrape{start_page}{end_page}\"\n",
    "    scrape_pages(list_of_links, start_page, end_page, csv_filename, table_name)\n",
    "    print(f\"Finished scraping pages {start_page} to {end_page} and saved to {csv_filename} and {table_name}.db\")\n",
    "    #print(\"Sleeping for 10 seconds before scraping the next pages...\")\n",
    "    time.sleep(1) # Sleep for 10 seconds before scraping the next pages"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
